{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from konlpy.tag import Komoran\n",
    "import random\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu와 cuda 중 다음 기기로 학습함: cuda\n"
     ]
    }
   ],
   "source": [
    "#GPU와 CPU 사용 설정\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"cpu와 cuda 중 다음 기기로 학습함:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#konlpy의 Komoran을 사용하여 텍스트와 레이블 토크나이즈(패딩도 자동으로 진행)\n",
    "tokenizer = Komoran()\n",
    "TEXT = data.Field(sequential=True,\n",
    "                  use_vocab=True,\n",
    "                  tokenize=tokenizer.morphs,\n",
    "                  lower=True,\n",
    "                  batch_first=True,\n",
    "                  fix_length=60)\n",
    "\n",
    "LABEL = data.Field(sequential=False,\n",
    "                   use_vocab=False,\n",
    "                   batch_first=False,\n",
    "                   is_target=True,\n",
    "                  dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data와 test data 분할\n",
    "train_data, test_data = TabularDataset.splits(path = '.', train = '악플_train.csv', test = '악플_test.csv', format = 'csv', fields = [('text', TEXT), ('label', LABEL)], skip_header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70537"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17633"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어 집합(중복을 제거한 총 단어들의 집합) 만들기\n",
    "TEXT.build_vocab(train_data, min_freq = 5, max_size = 20000) # min_freq = 최소 5번 이상 나온 단어만 단어 집합에 담는다. max_size = 단어 집합의 최대 사이즈 = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13773"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이퍼 파라미터 변수 정의\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련 데이터와 검증 데이터 분할\n",
    "train_data, val_data = train_data.split(split_ratio = 0.8)\n",
    "\n",
    "train_iter, valid_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train_data, val_data, test_data), batch_size=batch_size,\n",
    "        shuffle=True, repeat=False, device = DEVICE, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 미니 배치의 개수 : 441\n",
      "테스트 데이터의 미니 배치의 개수 : 138\n",
      "검증 데이터의 미니 배치의 개수 : 111\n"
     ]
    }
   ],
   "source": [
    "print('훈련 데이터의 미니 배치의 개수 : {}'.format(len(train_iter)))\n",
    "print('테스트 데이터의 미니 배치의 개수 : {}'.format(len(test_iter)))\n",
    "print('검증 데이터의 미니 배치의 개수 : {}'.format(len(valid_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, **model_config):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        if model_config['emb_type'] == 'glove' or 'fasttext':\n",
    "            self.emb = nn.Embedding(model_config['vocab_size'],\n",
    "                                   model_config['emb_dim'],\n",
    "                                   _weight = TEXT.vocab.vectors)\n",
    "        else:\n",
    "            self.emb = nn.Embedding(model_config['vocab_size'],\n",
    "                                   model_config['emb_dim'])\n",
    "            \n",
    "        self.bidirectional = model_config['bidirectional']\n",
    "        self.num_direction = 2 if model_config['bidirectional'] else 1\n",
    "        self.model_type = model_config['model_type']\n",
    "        \n",
    "        self.LSTM = nn.LSTM(input_size = model_config['emb_dim'],\n",
    "                           hidden_size = model_config['hidden_dim'],\n",
    "                           dropout = model_config['dropout'],\n",
    "                           bidirectional = model_config['bidirectional'],\n",
    "                           batch_first = model_config['batch_first'])\n",
    "        \n",
    "        self.fc = nn.Linear(model_config['hidden_dim'] * self.num_direction,\n",
    "                           model_config['output_dim'])\n",
    "        self.drop = nn.Dropout(model_config['dropout'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        output, (hidden, cell) = self.LSTM(emb)\n",
    "        last_output = output[:,-1,:]\n",
    "        \n",
    "        return self.fc(self.drop(last_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 128]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 128x60 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 128 (GPU 0)]\n",
      "tensor([[ 211,    2,   52,  ...,    1,    1,    1],\n",
      "        [   3,   20, 1248,  ...,    1,    1,    1],\n",
      "        [   0,  886,    1,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [  78,   12, 5418,  ...,    1,    1,    1],\n",
      "        [9875,    5,   73,  ...,    1,    1,    1],\n",
      "        [1334,  162,  225,  ...,    1,    1,    1]], device='cuda:0')\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sample_for_check = next(iter(train_iter))\n",
    "print(sample_for_check)\n",
    "print(sample_for_check.text)\n",
    "print(sample_for_check.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = dict(batch_first = True,\n",
    "                        model_type = 'LSTM',\n",
    "                        bidirectional = True,\n",
    "                        hidden_dim = 128,\n",
    "                        output_dim = 1,\n",
    "                        dropout = 0.8, #드롭아웃 비율 설정\n",
    "                   emb_type = '',\n",
    "                   vocab_size = len(TEXT.vocab),\n",
    "                   emb_dim = 300,\n",
    "                   batch_size = batch_size\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(**model_config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr, weight_decay=1e-5) #Adam(가중치 규제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.forward(sample_for_check.text).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(predictions, sample_for_check.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = binary_accuracy(predictions, sample_for_check.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1522, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, loss_fn, idx_Epoch, **model_params):\n",
    "    \n",
    "    Epoch_loss = 0\n",
    "    Epoch_acc = 0\n",
    "    model.train()\n",
    "    batch_size = model_params['batch_size']\n",
    "    \n",
    "    for idx, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(next(iter(batch))).squeeze()\n",
    "        loss = loss_fn(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        sys.stdout.write(\n",
    "        \"\\r\" + f\"[Train] Epoch: {idx_Epoch:^3}\"\\\n",
    "            f\"[{(idx + 1) * batch_size} / {len(iterator) * batch_size}({100. *(idx + 1) / len(iterator) :.4}%)]\"\\\n",
    "            f\"    Loss: {loss.item()}\"\\\n",
    "            f\"    Acc: {acc.item()}\"\\\n",
    "        )\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        Epoch_loss += loss.item()\n",
    "        Epoch_acc += acc.item()\n",
    "        \n",
    "    return Epoch_loss/len(iterator), Epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, loss_fn):\n",
    "    Epoch_loss = 0\n",
    "    Epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = loss_fn(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            \n",
    "            Epoch_loss += loss.item()\n",
    "            Epoch_acc += acc.item()\n",
    "    \n",
    "    return Epoch_loss/len(iterator), Epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCH = epochs\n",
    "best_valid_loss = float('inf')\n",
    "model_name = f\"{'bi' if model_config['bidirectional'] else ''}{model_config['model_type']}_{model_config['emb_type']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Model name: biLSTM_\n",
      "--------------------\n",
      "[Train] Epoch:  0 [56448 / 56448(100.0%)]    Loss: 0.1810069978237152    Acc: 0.9531255578427124\n",
      "\t Saved at 0-Epoch\n",
      "\t Epoch: 0 | Train Loss: -0.2424 | Train Acc: 0.9354\n",
      "\t Epoch: 0 | Valid Loss: 0.7992 | Valid Acc: 0.8174\n",
      "[Train] Epoch:  1 [56448 / 56448(100.0%)]    Loss: 0.19891096651554108    Acc: 0.92187551581421\n",
      "\t Saved at 1-Epoch\n",
      "\t Epoch: 1 | Train Loss: -0.3146 | Train Acc: 0.9402\n",
      "\t Epoch: 1 | Valid Loss: 0.7942 | Valid Acc: 0.8132\n",
      "[Train] Epoch:  2 [56448 / 56448(100.0%)]    Loss: 0.18903899192810059    Acc: 0.960937578684998\n",
      "\t Epoch: 2 | Train Loss: -0.7835 | Train Acc: 0.9457\n",
      "\t Epoch: 2 | Valid Loss: 0.8692 | Valid Acc: 0.819\n",
      "[Train] Epoch:  3 [56448 / 56448(100.0%)]    Loss: 0.22446675598621368    Acc: 0.960937565792847\n",
      "\t Epoch: 3 | Train Loss: -0.6433 | Train Acc: 0.9507\n",
      "\t Epoch: 3 | Valid Loss: 0.9246 | Valid Acc: 0.8063\n",
      "[Train] Epoch:  4 [56448 / 56448(100.0%)]    Loss: 0.23150768876075745    Acc: 0.929687570004272\n",
      "\t Epoch: 4 | Train Loss: -0.7768 | Train Acc: 0.9538\n",
      "\t Epoch: 4 | Valid Loss: 0.8869 | Valid Acc: 0.813\n",
      "[Train] Epoch:  5 [56448 / 56448(100.0%)]    Loss: 0.09017105400562286    Acc: 0.97656250399742126\n",
      "\t Epoch: 5 | Train Loss: -0.6221 | Train Acc: 0.9575\n",
      "\t Epoch: 5 | Valid Loss: 0.9257 | Valid Acc: 0.8077\n",
      "[Train] Epoch:  6 [56448 / 56448(100.0%)]    Loss: 0.13309137523174286    Acc: 0.9531255382896423\n",
      "\t Epoch: 6 | Train Loss: -1.023 | Train Acc: 0.9605\n",
      "\t Epoch: 6 | Valid Loss: 0.9269 | Valid Acc: 0.8139\n",
      "[Train] Epoch:  7 [56448 / 56448(100.0%)]    Loss: 0.17830368876457214    Acc: 0.9375875391319275\n",
      "\t Epoch: 7 | Train Loss: -1.094 | Train Acc: 0.9629\n",
      "\t Epoch: 7 | Valid Loss: 0.9489 | Valid Acc: 0.8137\n",
      "[Train] Epoch:  8 [56448 / 56448(100.0%)]    Loss: 0.15195634961128235    Acc: 0.9453125382896423\n",
      "\t Epoch: 8 | Train Loss: -0.8338 | Train Acc: 0.9641\n",
      "\t Epoch: 8 | Valid Loss: 0.9825 | Valid Acc: 0.8123\n",
      "[Train] Epoch:  9 [56448 / 56448(100.0%)]    Loss: 0.0705522745847702    Acc: 0.97656253387107849\n",
      "\t Epoch: 9 | Train Loss: -1.175 | Train Acc: 0.9677\n",
      "\t Epoch: 9 | Valid Loss: 1.066 | Valid Acc: 0.8122\n"
     ]
    }
   ],
   "source": [
    "print('-'*20)\n",
    "print(f'Model name: {model_name}')\n",
    "print('-'*20)\n",
    "\n",
    "for Epoch in range(N_EPOCH):\n",
    "    train_loss, train_acc = train(model, train_iter, optimizer, loss_fn, Epoch, **model_config)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iter, loss_fn)\n",
    "    print('')\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'./{model_name}.pt')\n",
    "        print(f'\\t Saved at {Epoch}-Epoch')\n",
    "    \n",
    "    print(f'\\t Epoch: {Epoch} | Train Loss: {train_loss:.4} | Train Acc: {train_acc:.4}')\n",
    "    print(f'\\t Epoch: {Epoch} | Valid Loss: {valid_loss:.4} | Valid Acc: {valid_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 4.041 | Test Acc: 0.8115\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iter, loss_fn)\n",
    "print('')\n",
    "print(f'Test Loss: {test_loss:.4} | Test Acc: {test_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
